import pandas as pd
import decimal
import os
from datetime import datetime, timedelta
import sys
import time

import config
import data_loaders
import data_transformers
import shopee_processor
import output_handlers
import pareto_analyzer
from dados_bling import mainbling


def add_missing_columns(df, all_cols_schema):
    """
    Adds missing columns to a DataFrame based on a schema with appropriate default values.
    """
    if df.empty:
        return pd.DataFrame(columns=all_cols_schema)

    for col in all_cols_schema:
        if col not in df.columns:
            if col in ['valor_total_produto', 'valor_unitario_venda', 'custo_total_produto', 'custo_unitario', 'cashback_cupom', 'Comiss√£o']:
                df[col] = decimal.Decimal('0.000')
            elif col in ['quantidade', 'Estq']:
                df[col] = 0
            elif col == 'hora_do_pedido':
                df[col] = ''
            elif col in ['Fornecedores', 'Categoria', 'Subcategoria', 'tipo_de_venda']:
                df[col] = pd.NA
            else:
                df[col] = ''
    return df[all_cols_schema]


if __name__ == "__main__":
    try:
        print(
            "\n--- [VERIFICA√á√ÉO] INICIANDO EXECU√á√ÉO DA VERS√ÉO OTIMIZADA BQ (mainvendas.py) ---")

        # 0. Initial Setup and Authentication
        data_loaders.autenticar_gcp()
        print("Iniciando o processo de carga e transforma√ß√£o de dados de vendas...")

        # Garante que as pastas de sa√≠da existam
        os.makedirs(config.PASTA_RELATORIOS_VENDAS, exist_ok=True)
        if os.path.dirname(config.ARQUIVO_BLING_PRODUTOS_CSV):
            os.makedirs(os.path.dirname(
                config.ARQUIVO_BLING_PRODUTOS_CSV), exist_ok=True)
        for zip_path in config.ARQUIVOS_SHOPEE_ZIP:
            if os.path.dirname(zip_path):
                os.makedirs(os.path.dirname(zip_path), exist_ok=True)

        # Call Bling data update process
        print("\n--- Iniciando atualiza√ß√£o dos dados do bling ---")
        mainbling.main()
        print("--- Processo atualiza√ß√£o do bling conclu√≠do ---")

        # Determine current month and year
        now = datetime.now()
        hoje = now.date()
        current_month_num = now.month
        current_year = now.year

        # ‚òÖ‚òÖ‚òÖ SEMPRE USAR MODO TRUNCATE ‚òÖ‚òÖ‚òÖ
        dia_do_mes = now.day
        # is_full_load = True # Sempre True para usar TRUNCATE
        print("\n*** üöÄ MODO DE CARGA: COMPLETA (WRITE_TRUNCATE) ***")
        print("   (A base inteira ser√° substitu√≠da)")

        month_name_map_pt = {
            1: 'janeiro', 2: 'fevereiro', 3: 'marco', 4: 'abril',
            5: 'maio', 6: 'junho', 7: 'julho', 8: 'agosto',
            9: 'setembro', 10: 'outubro', 11: 'novembro', 12: 'dezembro'
        }
        current_month_name_pt = month_name_map_pt.get(
            current_month_num, '').lower()

        all_possible_cols = list(dict.fromkeys(
            list(config.MAPEAMENTO_EXCEL_BIGQUERY.values()) +
            list(config.MAPEAMENTO_MAGIS5_BIGQUERY.values()) +
            ['Estq', 'Categoria', 'Subcategoria', 'Fornecedores', 'custo_unitario', 'custo_total_produto',
             'cashback_cupom', 'Comiss√£o', 'origem_dados', 'hora_do_pedido', 'tipo_de_venda']
        ))

        # --- L√ìGICA DE CARREGAMENTO DE DADOS ---

        # 1. Carrega dados de meses anteriores (SEMPRE, pois usamos TRUNCATE)
        df_vendas_excel_prev_months = pd.DataFrame()

        print(
            f"\nüì• Verificando arquivos Excel na pasta '{config.PASTA_RELATORIOS_VENDAS}' para meses anteriores...")
        excel_files_to_load = []
        if os.path.exists(config.PASTA_RELATORIOS_VENDAS):
            for filename in os.listdir(config.PASTA_RELATORIOS_VENDAS):
                if filename.endswith('.xlsx') or filename.endswith('.xls'):
                    file_base_name = os.path.splitext(filename)[0].lower()
                    if file_base_name != current_month_name_pt and not file_base_name.endswith('_processado'):
                        excel_files_to_load.append(os.path.join(
                            config.PASTA_RELATORIOS_VENDAS, filename))

        if excel_files_to_load:
            df_vendas_excel_prev_months = data_loaders.carregar_multiplos_excel_de_pasta(
                excel_files_to_load,
                list(config.MAPEAMENTO_EXCEL_BIGQUERY.keys()),
                config.MAPEAMENTO_EXCEL_BIGQUERY
            )
            df_vendas_excel_prev_months = add_missing_columns(
                df_vendas_excel_prev_months, all_possible_cols)
            print(
                f"‚úÖ Arquivos Excel de meses anteriores consolidados. Total de linhas: {len(df_vendas_excel_prev_months)}")
        else:
            print(
                "\n‚ö†Ô∏è Nenhum arquivo Excel de meses anteriores encontrado para carregar.")

        # 2. Carrega dados do m√™s vigente (Excel + API)
        print(
            f"\nüîÑ Processando dados para o m√™s vigente ({current_month_name_pt.capitalize()}/{current_year})...")
        dfs_mes_vigente = []

        current_month_excel_filepath = os.path.join(
            config.PASTA_RELATORIOS_VENDAS, f"{current_month_name_pt}.xlsx")

        # ‚òÖ‚òÖ‚òÖ L√ìGICA CORRIGIDA PARA DIA 1 vs OUTROS DIAS ‚òÖ‚òÖ‚òÖ
        if dia_do_mes == 1:
            # Primeiro dia do m√™s: API busca APENAS HOJE
            print(f"   üìÖ Primeiro dia do m√™s detectado.")
            api_data_inicio = hoje
            api_data_fim = hoje
            print(
                f"   - A API buscar√° apenas os dados de HOJE ({hoje.strftime('%d/%m/%Y')}).")

            # Carrega o Excel do m√™s anterior (se existir) para adicionar ao consolidado
            if os.path.exists(current_month_excel_filepath):
                print(
                    f"   - Arquivo Excel '{current_month_name_pt}.xlsx' encontrado (m√™s anterior).")
                df_excel_mes_vigente = data_loaders.carregar_multiplos_excel_de_pasta(
                    [current_month_excel_filepath],
                    list(config.MAPEAMENTO_EXCEL_BIGQUERY.keys()),
                    config.MAPEAMENTO_EXCEL_BIGQUERY
                )
                df_excel_mes_vigente = add_missing_columns(
                    df_excel_mes_vigente, all_possible_cols)
                dfs_mes_vigente.append(df_excel_mes_vigente)
                print(
                    f"   - Excel do m√™s anterior carregado: {len(df_excel_mes_vigente)} linhas.")
            else:
                print(
                    f"   - Nenhum Excel do m√™s anterior encontrado (normal no in√≠cio do primeiro m√™s).")

        else:
            # Qualquer outro dia: API busca ONTEM e HOJE
            print(f"   üìÖ Dia {dia_do_mes} do m√™s detectado.")

            if os.path.exists(current_month_excel_filepath):
                print(
                    f"   - Arquivo Excel '{current_month_name_pt}.xlsx' encontrado. Carregando dados...")
                df_excel_mes_vigente = data_loaders.carregar_multiplos_excel_de_pasta(
                    [current_month_excel_filepath],
                    list(config.MAPEAMENTO_EXCEL_BIGQUERY.keys()),
                    config.MAPEAMENTO_EXCEL_BIGQUERY
                )

                # Remove os dados de ONTEM e HOJE do Excel (ser√£o atualizados pela API)
                ontem = hoje - timedelta(days=1)
                api_data_inicio = ontem
                api_data_fim = hoje
                print(
                    f"   - A API buscar√° os dados de ONTEM e HOJE ({ontem.strftime('%d/%m/%Y')} a {hoje.strftime('%d/%m/%Y')}).")

                if not df_excel_mes_vigente.empty:
                    linhas_originais = len(df_excel_mes_vigente)

                    df_excel_mes_vigente['data_do_pedido'] = df_excel_mes_vigente['data_do_pedido'].astype(
                        str)
                    df_excel_mes_vigente['data_do_pedido'] = pd.to_datetime(
                        df_excel_mes_vigente['data_do_pedido'],
                        format='%d/%m/%Y %H:%M:%S',
                        errors='coerce'
                    ).dt.date

                    # Remove ONTEM e HOJE do Excel
                    df_excel_mes_vigente = df_excel_mes_vigente[
                        (df_excel_mes_vigente['data_do_pedido'] < ontem) |
                        (df_excel_mes_vigente['data_do_pedido'] > hoje)
                    ]

                    linhas_filtradas = len(df_excel_mes_vigente)
                    print(
                        f"   - Filtrando Excel: Removidas {linhas_originais - linhas_filtradas} linhas de ONTEM e HOJE (ser√£o atualizadas pela API).")

                df_excel_mes_vigente = add_missing_columns(
                    df_excel_mes_vigente, all_possible_cols)
                dfs_mes_vigente.append(df_excel_mes_vigente)

            else:
                # Se n√£o tem Excel, busca do dia 1 at√© hoje
                print(
                    f"   - Arquivo Excel '{current_month_name_pt}.xlsx' N√ÉO encontrado.")
                api_data_inicio = hoje.replace(day=1)
                api_data_fim = hoje
                print(
                    f"   - A API buscar√° os dados do in√≠cio do m√™s at√© hoje ({api_data_inicio.strftime('%d/%m/%Y')} a {api_data_fim.strftime('%d/%m/%Y')}).")
        # ‚òÖ‚òÖ‚òÖ FIM DA L√ìGICA CORRIGIDA ‚òÖ‚òÖ‚òÖ

        # Busca da API (sempre executa)
        df_vendas_api = data_loaders.carregar_vendas_magis5_api(
            config.MAGIS5_API_URL, config.MAGIS5_API_KEY, config.MAGIS5_PAGE_SIZE,
            config.MAPEAMENTO_MAGIS5_BIGQUERY,
            data_inicio=api_data_inicio,
            data_fim=api_data_fim
        )
        if not df_vendas_api.empty:
            df_vendas_api = add_missing_columns(
                df_vendas_api, all_possible_cols)
            dfs_mes_vigente.append(df_vendas_api)

        df_vendas_current_month_source = pd.DataFrame()
        if dfs_mes_vigente:
            df_vendas_current_month_source = pd.concat(
                dfs_mes_vigente, ignore_index=True)
            print(
                f"‚úÖ Dados do m√™s vigente (Excel + API) consolidados. Total de linhas: {len(df_vendas_current_month_source)}")
        else:
            print("‚ö†Ô∏è Nenhum dado do m√™s vigente (Excel ou API) foi carregado.")

        # 3. Combina√ß√£o final dos DataFrames
        print("\nüîÑ Combinando DataFrames de vendas de todos os per√≠odos...")
        if not df_vendas_excel_prev_months.empty or not df_vendas_current_month_source.empty:
            df_vendas = pd.concat(
                [df_vendas_excel_prev_months, df_vendas_current_month_source], ignore_index=True)
            print(
                f"‚úÖ DataFrames de todos os per√≠odos combinados. Total de linhas inicial: {len(df_vendas)}")
        else:
            print(
                "‚ùå Nenhum dado de vendas (Excel ou Magis5) foi carregado. Encerrando o script.")
            time.sleep(20)
            sys.exit(1)

        # --- FIM DA L√ìGICA DE CARREGAMENTO ---

        df_vendas = data_transformers.pre_processar_dataframe(df_vendas)

        print("\nüîó Carregando e mesclando dados de custo, estoque, fornecedores e categorias do Bling...")
        df_bling_data = data_loaders.carregar_dados_bling_csv()

        df_vendas = pd.merge(
            df_vendas,
            df_bling_data[['sku', 'custo_unitario', 'Estq', 'titulo_bling',
                           'Fornecedores', 'Categoria', 'Subcategoria', 'tipo_de_venda']],
            on='sku',
            how='left',
            suffixes=('_orig', '_bling')
        )

        for col_name in ['custo_unitario', 'Estq', 'titulo', 'Fornecedores', 'Categoria', 'Subcategoria', 'tipo_de_venda']:
            bling_col = f"{col_name}_bling" if col_name != 'titulo' else 'titulo_bling'
            orig_col = f"{col_name}_orig" if col_name != 'titulo' else 'titulo'

            if col_name == 'custo_unitario':
                df_vendas['custo_unitario'] = df_vendas[bling_col].combine_first(
                    df_vendas[orig_col]).apply(data_loaders.to_decimal_safe)
            elif col_name == 'Estq':
                df_vendas['Estq'] = df_vendas[bling_col].combine_first(
                    df_vendas[orig_col])
                df_vendas['Estq'] = pd.to_numeric(
                    df_vendas['Estq'], errors='coerce').fillna(0).astype(int)
            elif col_name == 'titulo':
                df_vendas['titulo'] = df_vendas['titulo'].fillna(
                    df_vendas[bling_col])
            else:
                df_vendas[col_name] = df_vendas[bling_col].combine_first(
                    df_vendas[orig_col] if orig_col in df_vendas.columns else pd.Series(dtype=object))
                df_vendas[col_name] = df_vendas[col_name].astype(
                    str).str.strip().fillna('')

        cols_to_drop_after_bling_merge = [
            col for col in df_vendas.columns if col.endswith(('_orig', '_bling'))]
        df_vendas = df_vendas.drop(
            columns=cols_to_drop_after_bling_merge, errors='ignore')

        print("‚úÖ Dados do Bling mesclados com sucesso.")

        # C√°lculo do custo total
        print("üí∞ Calculando 'custo_total_produto'...")
        df_vendas['quantidade'] = pd.to_numeric(
            df_vendas['quantidade'], errors='coerce').fillna(0)
        df_vendas['custo_unitario'] = df_vendas['custo_unitario'].apply(
            data_loaders.to_decimal_safe)
        df_vendas['custo_total_produto'] = (
            df_vendas['quantidade'] * df_vendas['custo_unitario']).apply(lambda x: decimal.Decimal(str(round(x, 3))))
        print("‚úÖ 'custo_total_produto' calculado com sucesso.")

        if not df_vendas_current_month_source.empty:
            df_current_month_processed = df_vendas[
                (pd.to_datetime(df_vendas['data_do_pedido']).dt.month == current_month_num) &
                (pd.to_datetime(
                    df_vendas['data_do_pedido']).dt.year == current_year)
            ].copy()
            df_current_month_processed = df_current_month_processed.drop(
                columns=['origem_dados'], errors='ignore')
            try:
                output_path_current_month_excel = os.path.join(
                    config.PASTA_RELATORIOS_VENDAS, f"{current_month_name_pt}_PROCESSADO.xlsx")
                df_current_month_processed.to_excel(
                    output_path_current_month_excel, index=False)
                print(
                    f"‚úÖ Dados processados do m√™s vigente exportados para '{output_path_current_month_excel}'.")
            except Exception as e:
                print(
                    f"‚ùå ERRO ao exportar dados processados do m√™s vigente para Excel: {e}")
        else:
            print("‚ö†Ô∏è Nenhum dado do m√™s vigente processado para exportar para Excel.")

        if 'origem_dados' in df_vendas.columns:
            df_vendas = df_vendas.drop(columns=['origem_dados'])

        print("üì¶ Processando relat√≥rios da Shopee...")
        df_vendas = shopee_processor.processar_relatorio_shopee(
            config.ARQUIVOS_SHOPEE_ZIP,
            df_vendas
        )

        print("\nüõ°Ô∏è Aplicando verifica√ß√£o final de duplicatas antes do upload...")
        linhas_antes_dedup_final = len(df_vendas)
        colunas_chave = ['numero_pedido', 'sku']
        if all(col in df_vendas.columns for col in colunas_chave):
            for col in colunas_chave:
                df_vendas[col] = df_vendas[col].astype(str).str.strip()

            df_vendas = df_vendas.drop_duplicates(
                subset=colunas_chave, keep='last')
            linhas_depois_dedup_final = len(df_vendas)
            removidas = linhas_antes_dedup_final - linhas_depois_dedup_final
            if removidas > 0:
                print(
                    f"‚úÖ Deduplica√ß√£o final conclu√≠da. {removidas} linhas duplicadas foram removidas.")
            else:
                print("‚úÖ Nenhuma duplicata encontrada na verifica√ß√£o final.")
        else:
            print(
                "‚ö†Ô∏è Colunas para deduplica√ß√£o ('numero_pedido', 'sku') n√£o encontradas. Pulando esta etapa.")

        print("üõ†Ô∏è Refor√ßando tipos e preenchimento para colunas finais...")
        if 'Estq' in df_vendas.columns:
            df_vendas['Estq'] = pd.to_numeric(
                df_vendas['Estq'], errors='coerce').fillna(0).astype(int)
        else:
            df_vendas['Estq'] = 0

        for col_str in ['Categoria', 'Subcategoria', 'Fornecedores', 'tipo_de_venda']:
            if col_str in df_vendas.columns:
                df_vendas[col_str] = df_vendas[col_str].astype(
                    str).str.strip().fillna('')
            else:
                df_vendas[col_str] = ''

        if df_vendas.columns.duplicated().any():
            print("‚ö†Ô∏è Aviso: Colunas duplicadas encontradas. Removendo duplicatas.")
            df_vendas = df_vendas.loc[:, ~
                                      df_vendas.columns.duplicated(keep='last')]

        print("üõ†Ô∏è Ajustando tipos de dados para o upload no BigQuery...")
        monetary_cols_to_convert = [
            'valor_total_produto', 'custo_total_produto', 'cashback_cupom', 'Comiss√£o', 'custo_unitario']
        for col in monetary_cols_to_convert:
            if col in df_vendas.columns:
                df_vendas[col] = df_vendas[col].apply(
                    data_loaders.to_decimal_safe)
            else:
                df_vendas[col] = decimal.Decimal('0.000')

        if 'data_do_pedido' in df_vendas.columns:
            df_vendas['data_do_pedido'] = pd.to_datetime(
                df_vendas['data_do_pedido'], errors='coerce').dt.date
        else:
            df_vendas['data_do_pedido'] = pd.NaT

        print("üìù Gerando arquivo Markdown para o dashboard...")
        output_handlers.gerar_saida_markdown(
            df_vendas, config.ARQUIVO_SAIDA_MD)

        print("‚¨ÜÔ∏è Preparando para upload no BigQuery...")
        colunas_finais_bigquery = [
            field.name for field in config.ESQUEMA_BIGQUERY]

        for col_schema in colunas_finais_bigquery:
            if col_schema not in df_vendas.columns:
                schema_field = next(
                    (field for field in config.ESQUEMA_BIGQUERY if field.name == col_schema), None)
                if schema_field:
                    if schema_field.field_type == "NUMERIC":
                        df_vendas[col_schema] = decimal.Decimal('0.000')
                    elif schema_field.field_type == "INTEGER":
                        df_vendas[col_schema] = 0
                    elif schema_field.field_type == "DATE":
                        df_vendas[col_schema] = pd.NaT
                    else:
                        df_vendas[col_schema] = ''

        df_vendas = df_vendas[colunas_finais_bigquery]

        # ‚òÖ‚òÖ‚òÖ SEMPRE USAR TRUNCATE ‚òÖ‚òÖ‚òÖ
        print("\nExecutando upload completo (TRUNCATE)...")
        output_handlers.fazer_upload_completo_bigquery(
            df_vendas,
            config.ID_PROJETO,
            config.ID_DATASET,
            config.ID_TABELA,
            config.ESQUEMA_BIGQUERY
        )

        print("\nüìä Executando a An√°lise de Pareto em mem√≥ria...")
        # Passa True para indicar que √© sempre full load (base completa)
        pareto_analyzer.analisar_pareto_por_loja(df_vendas.copy(), True)

        print("\n‚úÖ Processamento completo conclu√≠do com sucesso!")
        print("Programa finalizado. A janela fechar√° em 20 segundos...")
        time.sleep(20)

    except FileNotFoundError as e:
        print(
            f"‚ùå ERRO: Arquivo n√£o encontrado: {e}. Verifique se a pasta '{config.PASTA_RELATORIOS_VENDAS}' e os arquivos ZIP necess√°rios existem.")
        time.sleep(20)
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå ERRO inesperado durante o processamento principal: {e}")
        time.sleep(20)
        sys.exit(1)
